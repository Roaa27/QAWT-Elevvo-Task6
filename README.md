ğŸ“˜ Question Answering with Transformers

ğŸ“Œ Task Description

This project implements Question Answering (QA) using Transformer-based models fine-tuned on the SQuAD v1.1 dataset.
The system takes a context passage and a question as input, and extracts the correct answer span using pre-trained models (e.g., DistilBERT, RoBERTa).

â¸»

ğŸš€ Features
 â€¢ âœ… Load SQuAD v1.1 dataset (Stanford Question Answering Dataset).
 â€¢ âœ… Use Hugging Face Transformers for QA.
 â€¢ âœ… Pre-trained model: distilbert-base-uncased-distilled-squad.
 â€¢ âœ… Evaluation with Exact Match (EM) and F1 Score.
 â€¢ âœ… Bonus: Test alternative models like RoBERTa.
 â€¢ âœ… Simple Streamlit web interface to ask questions interactively.

â¸»

ğŸ› ï¸ Tools & Libraries
 â€¢ Transformers (https://huggingface.co/transformers/)
 â€¢ Datasets (https://huggingface.co/docs/datasets/)
 â€¢ Evaluate (https://huggingface.co/docs/evaluate/)
 â€¢ Streamlit (https://streamlit.io/)
 â€¢ Pandas (https://pandas.pydata.org/)
 â¸»

ğŸ“Š Evaluation Results

On a subset of 200 validation samples from SQuAD v1.1 using DistilBERT:
 â€¢ Exact Match (EM): ~85%
 â€¢ F1 Score: ~89.4
